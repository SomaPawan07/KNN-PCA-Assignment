{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems ?**\n",
        "- K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning algorithm. Here's what that means:\n",
        "   - Supervised learning: It requires a labeled dataset (each example has a known outcome).\n",
        "\n",
        "   - Non-parametric: It doesn't make assumptions about data distribution (like linearity or normality).\n",
        "\n",
        "   -  Instance-based (lazy learning): There is no explicit training phase. Instead, the algorithm stores all training data and makes decisions at prediction time using local information.\n",
        "Additionally, KNN is sometimes referred to as a lazy learner because it delays computation until it needs to make a prediction.\n",
        "\n",
        "**KNN for Classification**\n",
        "1. Identify k nearest neighbors\n",
        "2. Majority (or plurality) voting: Assign the class label that occurs most frequently among the neighbors.\n",
        "- In binary classification, using an odd value of k helps avoid ties.\n",
        "GeeksforGeeks\n",
        "MachineLearningMastery.com\n",
        "\n",
        "4. Optionally, apply weighted voting where nearer neighbors have more influence (e.g., weights based on inverse distance).\n",
        "\n",
        "**KNN for Regression**\n",
        "1. Identify k nearest neighbors\n",
        "2. Compute a prediction:\n",
        "- Simple average of neighbor values (common approach)\n",
        "- Weighted average by distance (gives closer points more weight)"
      ],
      "metadata": {
        "id": "PCckYxDFKLCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?**\n",
        "- The Curse of Dimensionality refers to a collection of problems that emerge when working with data in high-dimensional spaces—issues that aren't present in low-dimensional settings like 2D or 3D. Coined by Richard Bellman, these phenomena include exponential growth in data sparsity, computational complexity, and the breakdown of intuitive notions like “nearness” or similarity.\n",
        "\n",
        "It impacts KNN in the following ways:\n",
        "1. Distance Concentration & Uniformity:- As dimensionality grows, the difference between the nearest and farthest points shrinks—the distances become nearly identical. This makes it challenging for KNN to identify meaningful neighbors, since “closest” and “farthest” converge.\n",
        "\n",
        "2. Sparse Data, Exponential Growth Requirements:- High-dimensional spaces expand exponentially. To preserve the same density of data points, the dataset size has to grow dramatically—often infeasibly so. Otherwise, KNN struggles because there aren’t enough training points close enough to be reliable.\n",
        "\n",
        "3. Computational Burden:- KNN must compute distances between the query and all training points. In high dimensions, both the number of features and the volume of data increase, making these computations significantly more expensive.\n",
        "\n",
        "4. Breakdown of Data Structures (e.g., KD‑Trees):- Data structures like KD‑trees rely on spatial partitioning to accelerate nearest-neighbor search. But in high dimensions, almost all branches must be visited because points are so uniformly distributed—the structure becomes nearly as slow as brute-force search.\n",
        "\n",
        "5. Reduction in Predictive Effectiveness:- With distances becoming less meaningful, KNN loses its core assumption: that nearby points are similar. This leads to degraded performance in both classification and regression tasks.\n",
        "\n",
        "6. Emergence of “Hubs” in KNN Graphs:- In high-dimensional spaces, some data points disproportionately become neighbors for many others, forming “hubs.” This skewed distribution can distort results in classification and other KNN‐based tasks."
      ],
      "metadata": {
        "id": "6zMPk1xMMMef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection ?**\n",
        "- Principal Component Analysis (PCA) is a linear dimensionality reduction technique most commonly used for exploratory data analysis, visualization, and preprocessing.\n",
        "    -  It transforms the original features into a new coordinate system where the axes—called principal components—are orthogonal and ordered by the amount of variance they capture. The goal is to retain as much of the dataset's variability as possible in fewer dimensions.\n",
        "\n",
        "    - Mathematically, PCA computes the eigenvectors of the covariance (or correlation) matrix, and principal components correspond to the eigenvectors associated with the largest eigenvalues.\n",
        "\n",
        "    - The first few principal components capture most of the data’s intrinsic variability and can be used to reduce dimensions without significant information loss—making it easier to visualize or speed up further processing.\n",
        "\n",
        "The difference between PCA and feature selection are as follows:\n",
        "\n",
        "| Aspect               | PCA (Feature Extraction)                       | Feature Selection                              |\n",
        "| -------------------- | ---------------------------------------------- | ---------------------------------------------- |\n",
        "| Creates new features | Yes—combines existing features into components | No—selects subset of original features         |\n",
        "| Interpretability     | Lower—new features lose original meaning       | Higher—retains understandable variables        |\n",
        "| Goal                 | Capture maximum variance in fewer dimensions   | Retain the most relevant features for the task |\n",
        "| Technique type       | Unsupervised, mathematical transformation      | Supervised or unsupervised selection           |\n",
        "| Typical methods      | Eigen decomposition, linear projection         | RFE, LASSO, filter/wrapper/embedded methods    |\n"
      ],
      "metadata": {
        "id": "VnPgm6jLM-tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "- Eigenvectors are special non-zero vectors that, when transformed by a matrix (e.g., a covariance matrix), retain their direction; they may only be stretched or shrunk. Formally, for a matrix A and vector v, if\n",
        "Av=λv, then v is an eigenvector and λ its corresponding eigenvalue.\n",
        "- Eigenvalues (λ) are the scalars that quantify how much the transformation scales the eigenvector—how much it's stretched (|λ| > 1), shrunk (|λ| < 1), or reversed (if λ is negative).\n",
        "\n",
        "The eigenvalues and eogenvectors are important because:\n",
        "1. Identify Principal Directions:- Eigenvectors indicate the directions in which the data varies most intensely—these are your principal components.\n",
        "\n",
        "2. Quantify Importance with Eigenvalues:- Eigenvalues tell you how much variance each principal component captures. By ranking them, you determine which components are most informative.\n",
        "\n",
        "3. Decide How Many Components to Keep:- By examining eigenvalues (e.g., via a scree plot), you can choose the top components that collectively explain a major portion of the variance—this balances dimensionality reduction and information retention.\n",
        "\n",
        "4. Orthogonality Ensures Independence:- Eigenvectors in PCA are orthogonal (uncorrelated), meaning each principal component adds unique information."
      ],
      "metadata": {
        "id": "flsuVjpONzsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?**\n",
        "- 1. Tackles the Curse of Dimensionality: High-dimensional data dilutes the concept of distance—points become uniformly distant from each other, making KNN’s distance-based decisions unreliable.PCA mitigates this by projecting the data onto a lower-dimensional space, where distances regain meaning and KNN becomes more effective.\n",
        "2. Improves Accuracy: Numerous real-world studies show that PCA improves KNN performance:\n",
        "      - Using PCA on air quality data with KNN improved accuracy from about 90.74% to 93.06%, a 2.32% uplift\n",
        "\n",
        "      - In fish species classification with image features, combining PCA (for feature reduction) and KNN boosted accuracy by 7.5% compared to KNN alone\n",
        "      - An article on gas sensor data found that PCA significantly increased the AUC of a KNN classifier—from 0.822 to 0.979\n",
        "3. Speeds Up Computation: Reducing dimensions reduces both storage needs and distance computations. One sports project reported huge efficiency gains: reducing image feature space to just 50 PCA components made KNN significantly faster—even though accuracy trade-offs existed. Similarly, the MNIST/KNN pipeline saw efficiency and accuracy gains when reducing to 50–100 components, peaking around 97.76% accuracy\n",
        "\n",
        "4. Often Matches More Complex Nonlinear Methods: PCA can sometimes perform nearly as effectively as nonlinear alternatives like autoencoders—while being orders of magnitude faster. In evaluations on datasets like MNIST, KNN applied to PCA-transformed data matched the accuracy of autoencoder-based embeddings, with dramatically faster computation."
      ],
      "metadata": {
        "id": "jtAguqtoO-uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Dataset: Use the Wine Dataset from sklearn.datasets.load_wine().**\n",
        "\n",
        "**Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "Ans. 1. Goodboychan’s Example (StandardScaler): A hands‑on example using the Wine dataset demonstrates a clear benefit from scaling:\n",
        "- Without scaling, KNN achieved an accuracy of approximately 75.56%.\n",
        "- With StandardScaler, accuracy jumped to around 95.56%.\n",
        "\n",
        "2. K‑Nearest Neighbors on Wine Dataset with Scaling vs Unscaled (R Implementation)\n",
        "Another real‑world evaluation showcased a dramatic improvement:\n",
        "- Without scaling, average accuracy over repeated trials was around 74.36%.\n",
        "- With scaling, it soared to 93.82%.\n",
        "\n"
      ],
      "metadata": {
        "id": "pOWgInMhQksG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "acc_unscaled = knn.score(X_test, y_test)\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
        "pipeline.fit(X_train, y_train)\n",
        "acc_scaled = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_unscaled)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b1VVIaqZASS",
        "outputId": "ad8ddf0e-bd9f-429c-ec8a-f3018129fcda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n",
        "- To Compute Explained Variance Ratio on the Wine Dataset\n",
        "1. Load and scale the Wine dataset:"
      ],
      "metadata": {
        "id": "NmRjYOhSZNI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "qV7RR2XbZrMw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling is important—PCA is sensitive to feature scales; without it, high-variance features can disproportionately influence the components.\n",
        "\n",
        "2. Fit PCA to retain all components"
      ],
      "metadata": {
        "id": "J0ypxvLaZx7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=X.shape[1])  # 13 components for 13 original features\n",
        "pca.fit(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "-QvSlD7oZ0wj",
        "outputId": "11190c93-6068-4025-c78a-a65cb91e5130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=13)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=13)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>PCA</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=13)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Retrieve and display explained variance ratio"
      ],
      "metadata": {
        "id": "j5T58gzVZ7px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(explained_variance_ratio, start=1):\n",
        "    print(f\"PC{i}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdvODlDPZ-8Z",
        "outputId": "5709899c-5123-4244-86b1-f61abac87f22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.**\n",
        "- Here’s what I found regarding the comparison of KNN classifier performance on the Wine dataset with and without PCA (selecting top 2 principal components):\n",
        "Key Findings from Web Sources\n",
        "- A comparative experiment using 3 nearest neighbors (k=3) evaluated KNN on:\n",
        "1. The original feature set\n",
        "2. A PCA-transformed version using the first 6 PCs\n",
        "\n",
        "The recorded accuracies were:\n",
        "- Original dataset: approximately 79.4%\n",
        "- With 6 principal components: around 78.2%\n",
        "\n",
        "Although this study uses 6 PCs instead of 2, it provides solid insight: using reduced-dimensional data led to a slight drop in training accuracy, indicating that aggressive reduction can lose predictive information."
      ],
      "metadata": {
        "id": "y7hyn8KyZ3Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_original = knn.score(X_test_scaled, y_test)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca2 = knn_pca.score(X_test_pca, y_test)\n",
        "\n",
        "print(f\"Accuracy (original data): {acc_original:.3f}\")\n",
        "print(f\"Accuracy (PCA, 2 components): {acc_pca2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oltlIiHbacd",
        "outputId": "1d9a05b1-003a-4be9-bd94-171f41a67462"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (original data): 0.963\n",
            "Accuracy (PCA, 2 components): 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.**\n",
        "Ans. Insights from Available Analyses\n",
        "1. Empirical Results with Wine Dataset (Custom Implementation)\n",
        "A hands-on evaluation applying KNN (with both Euclidean and Manhattan distances) using nested cross-validation revealed:\n",
        "- Overall performance was consistently strong (~93–95%) across folds.\n",
        "- Manhattan distance slightly outperformed Euclidean distance in several cases.\n",
        "- Best recorded accuracy: 94.29% with k = 1 and Manhattan distance.\n",
        "\n",
        "2. Literature & Broader Comparisons\n",
        "Broader experimental research supports these findings:\n",
        "- Studies across multiple datasets found Euclidean and Manhattan distances often perform similarly, and generally outperform other options like Minkowski variants.\n",
        "- In a specific comparison using the Wine dataset, Manhattan distance yielded better accuracy than Euclidean."
      ],
      "metadata": {
        "id": "3w93L2dObl2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "results = {}\n",
        "for m in metrics:\n",
        "    knn = KNeighborsClassifier(n_neighbors=3, metric=m)\n",
        "    scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')\n",
        "    results[m] = (np.mean(scores), np.std(scores))\n",
        "\n",
        "for m, (mean_acc, std) in results.items():\n",
        "    print(f\"{m.title():<10}: Mean Accuracy = {mean_acc:.3f} ± {std:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUq849C6cmqM",
        "outputId": "68932a68-3592-4bf5-82b8-3958bcc6ff56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean : Mean Accuracy = 0.944 ± 0.040\n",
            "Manhattan : Mean Accuracy = 0.961 ± 0.038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.**\n",
        "\n",
        "**Explain how you would:**\n",
        "\n",
        "● **Use PCA to reduce dimensionality**\n",
        "\n",
        "● **Decide how many components to keep**\n",
        "\n",
        "● **Use KNN for classification post-dimensionality reduction**\n",
        "\n",
        "● **Evaluate the model**\n",
        "\n",
        "● **Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data**\n",
        "\n",
        "Ans. Here’s a clear and structured explanation—grounded in real-world biomedical research—on how you could tackle high-dimensional gene expression data (with many features but few samples) using a PCA → KNN pipeline:\n",
        "\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "- Problem: Gene expression datasets typically contain thousands of genes (features) but only tens to hundreds of patient samples. This disparity causes serious overfitting and computational burden.\n",
        "- Solution (PCA): Principal Component Analysis transforms the original correlated gene features into a smaller set of uncorrelated principal components, capturing the greatest variance in the data while discarding noise and redundancy.\n",
        "\n",
        "Steps:\n",
        "- Preprocess data: handle missing values (e.g., impute via nearest neighbors), and standardize genes to mean = 0, variance = 1.\n",
        "- Apply PCA, extracting components that represent the bulk of informative variance—thus creating a compact and meaningful feature representation.\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "- Criterion: Use the explained variance ratio to determine how many principal components capture, say, 95% of total variance.\n",
        "- Practical approach: Compute the cumulative explained variance and select the smallest number of components reaching your threshold (e.g., 95%)\n",
        "This ensures you’re preserving the most important patterns while greatly reducing dimensionality and mitigating overfitting.\n",
        "\n",
        "3. Use KNN for Classification After Dimensionality Reduction\n",
        "- Once dimensionality is reduced, apply K‑Nearest Neighbors in the new component space.\n",
        "- KNN benefits from PCA because:\n",
        "\n",
        "   -   It operates in a denser, lower-dimensional space, making distance computations meaningful again.\n",
        "\n",
        "   -   It’s less prone to overfitting and runs more efficiently.\n",
        "\n",
        "- Empirical studies in cancer classification show that PCA-based pipelines (e.g., with SVM or Random Forest) offer greater accuracy, precision, recall, and computational efficiency, reducing overfitting.\n",
        "\n",
        "4. Evaluate the Model Thoroughly\n",
        "\n",
        "To ensure robustness in a biomedical context:\n",
        "- Use cross-validation (e.g., k‑fold) to estimate accuracy with precision.\n",
        "- Evaluate multiple metrics, not just accuracy—e.g., precision, recall, F1-score, and AUC—since class imbalances may exist\n",
        "- For clinical relevance, deploy DET curves (Detection Error Trade-off) rather than just ROC curves, which helps assess performance across varying decision thresholds—important in personalized medicine settings.\n",
        "\n",
        "5. Justify the PCA + KNN Pipeline to Stakeholders\n",
        "\n",
        "| Benefit                                   | Explanation                                                                                                                                                         |\n",
        "| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Reduces overfitting**                   | PCA filters out noisy, redundant features, focusing on the most informative variance—this improves generalizability                               |\n",
        "| **Improves computational efficiency**     | Fewer dimensions mean faster training/prediction and lower memory usage                                                                           |\n",
        "| **Retains critical signal**               | PCA ensures that the retained components capture the core data structure, supporting accurate classification                                    |\n",
        "| **Transparent and interpretable process** | PCA followed by KNN is simpler and more explainable than complex models—crucial in biomedical research.                                                             |\n",
        "| **Clinically robust evaluation**          | Using advanced evaluation methods like DET curves ensures the model’s reliability across clinical thresholds                               |\n",
        "| **Proven effectiveness**                  | Case studies in cancer genomics consistently show improved model performance (accuracy, precision, recall, F1) with PCA preprocessing. |\n",
        "\n"
      ],
      "metadata": {
        "id": "EIc37COgcsvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsl_BsTeJ8aB"
      },
      "outputs": [],
      "source": []
    }
  ]
}